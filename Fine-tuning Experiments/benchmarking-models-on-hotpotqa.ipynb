{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13670431,"sourceType":"datasetVersion","datasetId":8692187},{"sourceId":13673557,"sourceType":"datasetVersion","datasetId":8694304},{"sourceId":13673658,"sourceType":"datasetVersion","datasetId":8694374},{"sourceId":13693035,"sourceType":"datasetVersion","datasetId":8709424}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Benchmarking DeBERTa-v3-large-squad2 fine tuned on hotpotQA for full answer (4 Layer Unfreeze)**","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:49:04.029145Z","iopub.execute_input":"2025-11-14T15:49:04.029492Z","iopub.status.idle":"2025-11-14T15:49:14.873952Z","shell.execute_reply.started":"2025-11-14T15:49:04.029470Z","shell.execute_reply":"2025-11-14T15:49:14.873229Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nCollecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow, evaluate\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.6 pyarrow-22.0.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\nfrom datasets import load_dataset\nimport evaluate\n\n# ------------------------------\n# Load model\n# ------------------------------\nmodel_path = \"/kaggle/input/deberta-v3-large-squad2-deepset-hotpot-fullanswer/kaggle/working/deberta-v3-large-squad-v2-hotpot\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_path)\n\nqa = pipeline(\n    \"question-answering\",\n    model=model,\n    tokenizer=tokenizer,\n    handle_impossible_answer=False,   # HotpotQA always has an answer\n    batch_size=16\n)\n\n# ------------------------------\n# Load HotpotQA dev set\n# ------------------------------\ndataset = load_dataset(\"hotpot_qa\", \"fullwiki\")\nhotpot_val = dataset[\"validation\"]\n\npredictions = []\nreferences = []\n\nfor ex in hotpot_val:\n    # Concat paragraphs only\n    paragraphs = [p[1] for p in ex[\"context\"]]\n    context = \"\\n\".join(paragraphs)\n\n    question = ex[\"question\"]\n    id_ = ex[\"id\"]\n\n    # Run QA model\n    try:\n        pred = qa({\n            \"context\": context,\n            \"question\": question\n        })\n        predicted_answer = pred.get(\"answer\", \"\").strip()\n    except:\n        predicted_answer = \"\"\n\n    # Prediction format\n    predictions.append({\n        \"id\": id_,\n        \"prediction_text\": predicted_answer,\n        \"no_answer_probability\": 0.0   # not used but expected by squad_v2 metric\n    })\n\n    # Reference format\n    references.append({\n        \"id\": id_,\n        \"answers\": {\n            \"text\": [ex[\"answer\"]],\n            \"answer_start\": [0]       # dummy start (HotpotQA does not provide span)\n        }\n    })\n\n# ------------------------------\n# Compute metrics (use squad_v2)\n# ------------------------------\nmetric = evaluate.load(\"squad_v2\")\nresults = metric.compute(predictions=predictions, references=references)\n\nprint(\"HotpotQA Exact Match (EM):\", results[\"exact\"])\nprint(\"HotpotQA F1:\", results[\"f1\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:49:14.875531Z","iopub.execute_input":"2025-11-14T15:49:14.875757Z","iopub.status.idle":"2025-11-14T15:56:03.641988Z","shell.execute_reply.started":"2025-11-14T15:49:14.875730Z","shell.execute_reply":"2025-11-14T15:56:03.641282Z"}},"outputs":[{"name":"stderr","text":"2025-11-14 15:49:31.979327: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763135372.471973      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763135372.583231      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd28c3196e19405c93ebbec1ef1558c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"fullwiki/train-00000-of-00002.parquet:   0%|          | 0.00/166M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"788f70573cfd4cda9bc49e2d451123af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"fullwiki/train-00001-of-00002.parquet:   0%|          | 0.00/166M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d268022856d6452c8d55373de8e1a80e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"fullwiki/validation-00000-of-00001.parqu(…):   0%|          | 0.00/28.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46055a29f70f4f0ba8425aaf37170a52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"fullwiki/test-00000-of-00001.parquet:   0%|          | 0.00/27.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52fb2300bbe94d558fcc0b98acd4d262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a8facbac0ef43a9b6d9f6060e3060f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2da92788d5484f75a9d86ad0cb54320c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7405 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f7a8c4c23054bb683b1ecf1132ec817"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n  warnings.warn(\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1fc1acf39bd4d1c8dc3be0a635bbfa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85e40b1bf3f04176b0434a2969dda17f"}},"metadata":{}},{"name":"stdout","text":"HotpotQA Exact Match (EM): 0.0\nHotpotQA F1: 0.1438456994321474\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Benchmarking DeBERTa-v3-large-squad2 fine tuned on hotpotQA for gold standard Paragraph**","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\nfrom datasets import load_dataset\nimport evaluate\n\nmodel_path = \"/kaggle/input/deberta-v3-large-squad2-finetune-hotpot-goldpara/debarta-squad-hotpot-final/checkpoint-2000\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_path)\n\nqa = pipeline(\n    \"question-answering\",\n    model=model,\n    tokenizer=tokenizer,\n    handle_impossible_answer=False,   # HotpotQA has no null answers\n    batch_size=16\n)\n\ndataset = load_dataset(\"hotpot_qa\", \"fullwiki\")\nhotpot_val = dataset[\"validation\"]\n\npredictions = []\nreferences = []\n\nfor ex in hotpot_val:\n    paragraphs = [p[1] for p in ex[\"context\"]]\n    context = \"\\n\".join(paragraphs)\n\n    question = ex[\"question\"]\n    id_ = ex[\"id\"]\n\n    try:\n        pred = qa({\n            \"context\": context,\n            \"question\": question\n        })\n        predicted_answer = pred.get(\"answer\", \"\").strip()\n    except:\n        predicted_answer = \"\"\n\n    # PREDICTION FORMAT\n    predictions.append({\n        \"id\": id_,\n        \"prediction_text\": predicted_answer,\n        \"no_answer_probability\": 0.0\n    })\n\n    # REFERENCE FORMAT — MUST include no_answer_probability\n    references.append({\n        \"id\": id_,\n        \"answers\": {\n            \"text\": [ex[\"answer\"]],\n            \"answer_start\": [0]\n        }\n    })\n\nmetric = evaluate.load(\"squad_v2\")\nresults = metric.compute(predictions=predictions, references=references)\n\nprint(\"HotpotQA Exact Match (EM):\", results[\"exact\"])\nprint(\"HotpotQA F1:\", results[\"f1\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:56:03.642807Z","iopub.execute_input":"2025-11-14T15:56:03.643085Z","iopub.status.idle":"2025-11-14T16:01:58.405472Z","shell.execute_reply.started":"2025-11-14T15:56:03.643055Z","shell.execute_reply":"2025-11-14T16:01:58.404702Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"HotpotQA Exact Match (EM): 0.0\nHotpotQA F1: 0.11803478987813897\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **Benchmarking DeBERTa-v3-large fine tune on SQuAD 2.0 (3/4)**","metadata":{}},{"cell_type":"markdown","source":"**For only .pt file model**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, DebertaV2ForQuestionAnswering\n\n# Load saved state dict\nstate = torch.load(\"/kaggle/input/deberta-v3-large-finetune-squad2-0/step24648_epoch0.pt\")\n\n# Load base model and tokenizer\nmodel = DebertaV2ForQuestionAnswering.from_pretrained(\"microsoft/deberta-v3-large\")\nmodel.load_state_dict(state[\"model\"], strict=False)\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n\n\nqa = pipeline(\n    \"question-answering\",\n    model=model,\n    tokenizer=tokenizer,\n    handle_impossible_answer=False,      # HotpotQA always has an answer\n    batch_size=16\n)\n\n# ------------------------------\n# Load HotpotQA dev set\n# ------------------------------\ndataset = load_dataset(\"hotpot_qa\", \"fullwiki\")\nhotpot_val = dataset[\"validation\"]\n\npredictions = []\nreferences = []\n\nfor ex in hotpot_val:\n    # HotpotQA context is list of [title, paragraph] pairs → concat only paragraphs\n    paragraphs = [p[1] for p in ex[\"context\"]]\n    context = \"\\n\".join(paragraphs)\n\n    question = ex[\"question\"]\n    id_ = ex[\"id\"]\n\n    # Run QA model\n    try:\n        pred = qa({\n            \"context\": context,\n            \"question\": question\n        })\n        predicted_answer = pred.get(\"answer\", \"\").strip()\n    except:\n        predicted_answer = \"\"\n\n    predictions.append({\n        \"id\": id_,\n        \"prediction_text\": predicted_answer,\n        \"no_answer_probability\": 0.0\n    })\n\n    references.append({\n        \"id\": id_,\n        \"answers\": {\"text\": [ex[\"answer\"]], \"answer_start\": [0]}\n    })\n\n# ------------------------------\n# Compute metrics\n# ------------------------------\nmetric = evaluate.load(\"squad_v2\")\nresults = metric.compute(predictions=predictions, references=references)\n\nprint(\"HotpotQA Exact Match:\", results[\"exact\"])\nprint(\"HotpotQA F1:\", results[\"f1\"])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:36:38.673834Z","iopub.execute_input":"2025-11-14T16:36:38.674140Z","iopub.status.idle":"2025-11-14T16:42:15.262703Z","shell.execute_reply.started":"2025-11-14T16:36:38.674118Z","shell.execute_reply":"2025-11-14T16:42:15.261834Z"}},"outputs":[{"name":"stderr","text":"Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"HotpotQA Exact Match: 0.0\nHotpotQA F1: 0.1408875951911262\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# **DeBERTa-v3-large-squad2**","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\nfrom datasets import load_dataset\nimport evaluate\n\n# ------------------------------\n# Load model\n# ------------------------------\nmodel_path = \"deepset/deberta-v3-large-squad2\"   # or your fine-tuned checkpoint\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_path)\n\nqa = pipeline(\n    \"question-answering\",\n    model=model,\n    tokenizer=tokenizer,\n    handle_impossible_answer=False,      # HotpotQA always has an answer\n    batch_size=16\n)\n\n# ------------------------------\n# Load HotpotQA dev set\n# ------------------------------\ndataset = load_dataset(\"hotpot_qa\", \"fullwiki\")\nhotpot_val = dataset[\"validation\"]\n\npredictions = []\nreferences = []\n\nfor ex in hotpot_val:\n    # HotpotQA context is list of [title, paragraph] pairs → concat only paragraphs\n    paragraphs = [p[1] for p in ex[\"context\"]]\n    context = \"\\n\".join(paragraphs)\n\n    question = ex[\"question\"]\n    id_ = ex[\"id\"]\n\n    # Run QA model\n    try:\n        pred = qa({\n            \"context\": context,\n            \"question\": question\n        })\n        predicted_answer = pred.get(\"answer\", \"\").strip()\n    except:\n        predicted_answer = \"\"\n\n    predictions.append({\n        \"id\": id_,\n        \"prediction_text\": predicted_answer\n    })\n\n    references.append({\n        \"id\": id_,\n        \"answers\": {\"text\": [ex[\"answer\"]], \"answer_start\": [0]}\n    })\n\n# ------------------------------\n# Compute metrics\n# ------------------------------\nmetric = evaluate.load(\"hotpot_qa\")\nresults = metric.compute(predictions=predictions, references=references)\n\nprint(\"HotpotQA Exact Match:\", results[\"exact_match\"])\nprint(\"HotpotQA F1:\", results[\"f1\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:08:26.199843Z","iopub.status.idle":"2025-11-14T16:08:26.200131Z","shell.execute_reply.started":"2025-11-14T16:08:26.199969Z","shell.execute_reply":"2025-11-14T16:08:26.199979Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **BigBird-RoBERTa-large fine tuned on MASHQA**","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\nfrom datasets import load_dataset\nimport evaluate\n\n# ------------------------------\n# Load model\n# ------------------------------\nmodel_path = \"/kaggle/input/bigbird-finetuned-mashqa/bigbird-roberta-large-mashqa\"   # or your fine-tuned checkpoint\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_path)\n\nqa = pipeline(\n    \"question-answering\",\n    model=model,\n    tokenizer=tokenizer,\n    handle_impossible_answer=False,      # HotpotQA always has an answer\n    batch_size=16\n)\n\n# ------------------------------\n# Load HotpotQA dev set\n# ------------------------------\ndataset = load_dataset(\"hotpot_qa\", \"fullwiki\")\nhotpot_val = dataset[\"validation\"]\n\npredictions = []\nreferences = []\n\nfor ex in hotpot_val:\n    # HotpotQA context is list of [title, paragraph] pairs → concat only paragraphs\n    paragraphs = [p[1] for p in ex[\"context\"]]\n    context = \"\\n\".join(paragraphs)\n\n    question = ex[\"question\"]\n    id_ = ex[\"id\"]\n\n    # Run QA model\n    try:\n        pred = qa({\n            \"context\": context,\n            \"question\": question\n        })\n        predicted_answer = pred.get(\"answer\", \"\").strip()\n    except:\n        predicted_answer = \"\"\n\n    predictions.append({\n        \"id\": id_,\n        \"prediction_text\": predicted_answer\n    })\n\n    references.append({\n        \"id\": id_,\n        \"answers\": {\"text\": [ex[\"answer\"]], \"answer_start\": [0]}\n    })\n\n# ------------------------------\n# Compute metrics\n# ------------------------------\nmetric = evaluate.load(\"hotpot_qa\")\nresults = metric.compute(predictions=predictions, references=references)\n\nprint(\"HotpotQA Exact Match:\", results[\"exact_match\"])\nprint(\"HotpotQA F1:\", results[\"f1\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:08:26.201594Z","iopub.status.idle":"2025-11-14T16:08:26.201841Z","shell.execute_reply.started":"2025-11-14T16:08:26.201732Z","shell.execute_reply":"2025-11-14T16:08:26.201743Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **BigBird-RoBERTa-large**","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\nfrom datasets import load_dataset\nimport evaluate\n\n# ------------------------------\n# Load model\n# ------------------------------\nmodel_path = \"google/bigbird-roberta-large\"   # or your fine-tuned checkpoint\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_path)\n\nqa = pipeline(\n    \"question-answering\",\n    model=model,\n    tokenizer=tokenizer,\n    handle_impossible_answer=False,      # HotpotQA always has an answer\n    batch_size=16\n)\n\n# ------------------------------\n# Load HotpotQA dev set\n# ------------------------------\ndataset = load_dataset(\"hotpot_qa\", \"fullwiki\")\nhotpot_val = dataset[\"validation\"]\n\npredictions = []\nreferences = []\n\nfor ex in hotpot_val:\n    # HotpotQA context is list of [title, paragraph] pairs → concat only paragraphs\n    paragraphs = [p[1] for p in ex[\"context\"]]\n    context = \"\\n\".join(paragraphs)\n\n    question = ex[\"question\"]\n    id_ = ex[\"id\"]\n\n    # Run QA model\n    try:\n        pred = qa({\n            \"context\": context,\n            \"question\": question\n        })\n        predicted_answer = pred.get(\"answer\", \"\").strip()\n    except:\n        predicted_answer = \"\"\n\n    predictions.append({\n        \"id\": id_,\n        \"prediction_text\": predicted_answer\n    })\n\n    references.append({\n        \"id\": id_,\n        \"answers\": {\"text\": [ex[\"answer\"]], \"answer_start\": [0]}\n    })\n\n# ------------------------------\n# Compute metrics\n# ------------------------------\nmetric = evaluate.load(\"hotpot_qa\")\nresults = metric.compute(predictions=predictions, references=references)\n\nprint(\"HotpotQA Exact Match:\", results[\"exact_match\"])\nprint(\"HotpotQA F1:\", results[\"f1\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:08:26.203468Z","iopub.status.idle":"2025-11-14T16:08:26.203756Z","shell.execute_reply.started":"2025-11-14T16:08:26.203611Z","shell.execute_reply":"2025-11-14T16:08:26.203624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}