{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13279065,"sourceType":"datasetVersion","datasetId":8415527}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 0 — (optional) show GPU & free memory\n!nvidia-smi -L\n!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T11:42:18.127220Z","iopub.execute_input":"2025-10-06T11:42:18.127467Z","iopub.status.idle":"2025-10-06T11:42:18.490751Z","shell.execute_reply.started":"2025-10-06T11:42:18.127449Z","shell.execute_reply":"2025-10-06T11:42:18.490094Z"}},"outputs":[{"name":"stdout","text":"GPU 0: Tesla T4 (UUID: GPU-b389e076-f280-a708-c183-fde045298406)\nGPU 1: Tesla T4 (UUID: GPU-bb3f6682-368e-2d69-73e3-6ad0bc9c096e)\nMon Oct  6 11:42:18 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   36C    P8             11W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   44C    P8             12W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CELL 1 — install required packages (run once)\n# use -q to reduce noise; versions can be changed if needed\n!pip install -q transformers accelerate bitsandbytes datasets peft evaluate safetensors\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T11:42:36.730245Z","iopub.execute_input":"2025-10-06T11:42:36.730905Z","iopub.status.idle":"2025-10-06T11:44:07.931560Z","shell.execute_reply.started":"2025-10-06T11:42:36.730859Z","shell.execute_reply":"2025-10-06T11:44:07.930687Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# CELL 2 — imports + reproducibility\nimport os, re, random, math, json, gc\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T11:45:27.334109Z","iopub.execute_input":"2025-10-06T11:45:27.334395Z","iopub.status.idle":"2025-10-06T11:46:07.504728Z","shell.execute_reply.started":"2025-10-06T11:45:27.334368Z","shell.execute_reply":"2025-10-06T11:46:07.503893Z"}},"outputs":[{"name":"stderr","text":"2025-10-06 11:45:48.934353: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759751149.286259      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759751149.393695      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# CELL 3 — load training_set_rel3.tsv\nimport pandas as pd, os\n\nDATA_PATH = \"/kaggle/input/training-set/training_set_rel3.tsv\"  # adjust if path differs\ndf = pd.read_csv(DATA_PATH, sep=\"\\t\", encoding=\"latin-1\")\n\nprint(\"Columns:\", df.columns.tolist())\nprint(\"Total essays:\", len(df))\n\n# keep only relevant columns\ncols_needed = [\"essay_id\", \"essay_set\", \"essay\", \"domain1_score\"]\ndf = df[cols_needed]\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T11:56:42.816349Z","iopub.execute_input":"2025-10-06T11:56:42.816634Z","iopub.status.idle":"2025-10-06T11:56:43.111167Z","shell.execute_reply.started":"2025-10-06T11:56:42.816611Z","shell.execute_reply":"2025-10-06T11:56:43.110585Z"}},"outputs":[{"name":"stdout","text":"Columns: ['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1', 'rater3_domain1', 'domain1_score', 'rater1_domain2', 'rater2_domain2', 'domain2_score', 'rater1_trait1', 'rater1_trait2', 'rater1_trait3', 'rater1_trait4', 'rater1_trait5', 'rater1_trait6', 'rater2_trait1', 'rater2_trait2', 'rater2_trait3', 'rater2_trait4', 'rater2_trait5', 'rater2_trait6', 'rater3_trait1', 'rater3_trait2', 'rater3_trait3', 'rater3_trait4', 'rater3_trait5', 'rater3_trait6']\nTotal essays: 12976\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   essay_id  essay_set                                              essay  \\\n0         1          1  Dear local newspaper, I think effects computer...   \n1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n4         5          1  Dear @LOCATION1, I know having computers has a...   \n\n   domain1_score  \n0              8  \n1              9  \n2              7  \n3             10  \n4              8  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>essay_set</th>\n      <th>essay</th>\n      <th>domain1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Dear local newspaper, I think effects computer...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>Dear @LOCATION1, I know having computers has a...</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# CELL 4 — select only Essay Set 1 and basic cleanup\nESSAY_TOPIC = (\n    \"More and more people use computers, but not everyone agrees that this benefits society. Those who support advances in technology believe that computers have a positive effect on people. They teach hand-eye coordination, give people the ability to learn about far away places and people, and even allow people to talk online with other people. Others have different ideas. Some experts are concerned that people are spending too much time on their computers and less time exercising, enjoying nature, and interacting with family and friends. Write a letter to your local newspaper in which you state your opinion on the effects computers have on people. Persuade the readers to agree with you.\"\n)\n\ndf = df[df[\"essay_set\"] == 1].copy()\ndf = df.dropna(subset=[\"essay\", \"domain1_score\"]).reset_index(drop=True)\n\ndf[\"essay\"] = df[\"essay\"].astype(str).str.strip()\ndf[\"domain1_score\"] = df[\"domain1_score\"].astype(int)\n\nprint(\"Essay Set 1 samples:\", len(df))\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T11:58:39.903042Z","iopub.execute_input":"2025-10-06T11:58:39.903319Z","iopub.status.idle":"2025-10-06T11:58:39.924969Z","shell.execute_reply.started":"2025-10-06T11:58:39.903302Z","shell.execute_reply":"2025-10-06T11:58:39.924173Z"}},"outputs":[{"name":"stdout","text":"Essay Set 1 samples: 1783\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   essay_id  essay_set                                              essay  \\\n0         1          1  Dear local newspaper, I think effects computer...   \n1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n4         5          1  Dear @LOCATION1, I know having computers has a...   \n\n   domain1_score  \n0              8  \n1              9  \n2              7  \n3             10  \n4              8  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>essay_set</th>\n      <th>essay</th>\n      <th>domain1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Dear local newspaper, I think effects computer...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>Dear @LOCATION1, I know having computers has a...</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"train_df, temp_df = train_test_split(df, test_size=0.30, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T12:02:39.112576Z","iopub.execute_input":"2025-10-06T12:02:39.113469Z","iopub.status.idle":"2025-10-06T12:02:39.120212Z","shell.execute_reply.started":"2025-10-06T12:02:39.113440Z","shell.execute_reply":"2025-10-06T12:02:39.119438Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(len(train_df))\nprint(len(val_df))\nprint(len(test_df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T12:03:13.844057Z","iopub.execute_input":"2025-10-06T12:03:13.844593Z","iopub.status.idle":"2025-10-06T12:03:13.848445Z","shell.execute_reply.started":"2025-10-06T12:03:13.844572Z","shell.execute_reply":"2025-10-06T12:03:13.847797Z"}},"outputs":[{"name":"stdout","text":"1248\n267\n268\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# CELL 6 — convert to HuggingFace Datasets\ntrain_ds = Dataset.from_pandas(train_df)\nval_ds = Dataset.from_pandas(val_df)\ntest_ds = Dataset.from_pandas(test_df)\ndataset = DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T12:03:46.114495Z","iopub.execute_input":"2025-10-06T12:03:46.115244Z","iopub.status.idle":"2025-10-06T12:03:46.184654Z","shell.execute_reply.started":"2025-10-06T12:03:46.115218Z","shell.execute_reply":"2025-10-06T12:03:46.183891Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# CELL 7 — prompt + label creation (uses essay topic as context)\n\nSYSTEM_PROMPT = (\n    \"You are an experienced English teacher grading a student's essay.\\n\\n\"\n    \"Read the essay carefully and assign a score \\n\\n\"\n    \"Essay Prompt:\\n\"\n    f\"{ESSAY_TOPIC}\\n\\n\"\n    \"Instructions:\\n\"\n    \"- The score must be a single integer between 2 and 12 (inclusive).\\n\"\n    \"- Do not provide any explanation or reasoning.\\n\"\n    \"- Output only the score number.\\n\\n\"\n    \"Now, read the following essay and provide the score.\\n\"\n)\n\nPROMPT_SUFFIX = \"\\n\\nEssay:\\n\"\nSCORE_SUFFIX = \"\\n\\nScore: \"\n\n\ndef make_full_input(example):\n    essay = example[\"essay\"].strip()\n    score = str(int(example[\"domain1_score\"]))\n    prompt = SYSTEM_PROMPT + PROMPT_SUFFIX + essay + SCORE_SUFFIX\n    full = prompt + score\n    return {\"prompt\": prompt, \"full\": full, \"score_str\": score}\n\ndataset = dataset.map(lambda x: make_full_input(x), remove_columns=dataset[\"train\"].column_names)\ndataset = dataset.rename_columns({\"prompt\": \"prompt_text\", \"full\": \"text\", \"score_str\": \"score_text\"})\ndataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T12:19:39.262229Z","iopub.execute_input":"2025-10-06T12:19:39.262767Z","iopub.status.idle":"2025-10-06T12:19:39.508465Z","shell.execute_reply.started":"2025-10-06T12:19:39.262735Z","shell.execute_reply":"2025-10-06T12:19:39.507887Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1248 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"955a3c5ef75844e98cc75ef6ae2e9edf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/267 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac828a0dca2041579733f538a790a237"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15e534c1e1154fd099fc582935cf4245"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt_text', 'text', 'score_text'],\n        num_rows: 1248\n    })\n    validation: Dataset({\n        features: ['prompt_text', 'text', 'score_text'],\n        num_rows: 267\n    })\n    test: Dataset({\n        features: ['prompt_text', 'text', 'score_text'],\n        num_rows: 268\n    })\n})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# CELL 8 — tokenizer + tokenization mapping\nMODEL_NAME = \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\"  # prefer a pre-quantized community HF repo if available; fallback below\n# fallback: \"Qwen/Qwen2.5-3B\"\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, use_fast=False)\nexcept Exception as e:\n    print(\"Primary model tokeniser failed; falling back to official name. Error:\", e)\n    MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, use_fast=False)\n\n# Ensure padding token exists:\nif tokenizer.pad_token_id is None:\n    tokenizer.add_special_tokens({\"pad_token\":\"<|pad|>\"})\n\nMAX_LENGTH = 1024  # keep moderate for VRAM; Qwen supports long contexts, but T4 is limited\nprint(\"Tokenizer loaded. Pad token id:\", tokenizer.pad_token_id)\n\n# tokenization function that produces input_ids and labels (masking prompt tokens)\ndef tokenize_and_mask(examples):\n    outputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n    for prompt, full in zip(examples[\"prompt_text\"], examples[\"text\"]):\n        tokenized_full = tokenizer(full, truncation=True, max_length=MAX_LENGTH)\n        tokenized_prompt = tokenizer(prompt, truncation=True, max_length=MAX_LENGTH)\n        input_ids = tokenized_full[\"input_ids\"]\n        attention_mask = tokenized_full[\"attention_mask\"]\n        labels = input_ids.copy()\n        # mask the prompt tokens\n        prompt_len = len(tokenized_prompt[\"input_ids\"])\n        labels[:prompt_len] = [-100] * prompt_len\n        outputs[\"input_ids\"].append(input_ids)\n        outputs[\"attention_mask\"].append(attention_mask)\n        outputs[\"labels\"].append(labels)\n    return outputs\n\n# apply tokenization\ntokenized = dataset.map(tokenize_and_mask, batched=True, remove_columns=dataset[\"train\"].column_names)\n# make sure data are in torch format at training time via collator\ntokenized\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T12:22:02.917568Z","iopub.execute_input":"2025-10-06T12:22:02.918336Z","iopub.status.idle":"2025-10-06T12:22:26.919460Z","shell.execute_reply.started":"2025-10-06T12:22:02.918305Z","shell.execute_reply":"2025-10-06T12:22:26.918888Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"332b92c486664be0a56a0e9e993f77ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2a193d01e2749fdb2ea27e0b2fb036c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceadd0f055404eee9c41fd92cbcd712d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"247faab668834996803f25f0781d7bd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ae3f5ee453d4ea2ae5912afa514651a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7b2162d0c04412f9d711f5a094ff3ff"}},"metadata":{}},{"name":"stdout","text":"Tokenizer loaded. Pad token id: 151654\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1248 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7745108862d4483dba42b5fe2a891355"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/267 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93061d03276b4568a59a421733e5c93a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa8d01830e434edfa3363ae79b851131"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 1248\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 267\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 268\n    })\n})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# CELL 9 — small custom collator to pad labels with -100\nimport torch\ndef data_collator(batch):\n    input_ids = [torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in batch]\n    labels = [torch.tensor(x[\"labels\"], dtype=torch.long) for x in batch]\n    attention = [torch.tensor(x[\"attention_mask\"], dtype=torch.long) for x in batch]\n    input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n    attention_padded = torch.nn.utils.rnn.pad_sequence(attention, batch_first=True, padding_value=0)\n    return {\"input_ids\": input_ids_padded, \"attention_mask\": attention_padded, \"labels\": labels_padded}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T12:23:40.021931Z","iopub.execute_input":"2025-10-06T12:23:40.022225Z","iopub.status.idle":"2025-10-06T12:23:40.028064Z","shell.execute_reply.started":"2025-10-06T12:23:40.022206Z","shell.execute_reply":"2025-10-06T12:23:40.027288Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# CELL 10 — load the model in 4-bit with bitsandbytes config (QLoRA style)\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",            # nf4 is common for QLoRA\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nprint(\"Loading model (this can take a minute)...\")\ntry:\n    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,\n                                                 quantization_config=bnb_config,\n                                                 trust_remote_code=True,\n                                                 device_map=\"auto\")\nexcept Exception as e:\n    # fallback: try official repo\n    print(\"Primary load failed, trying official Qwen name. Err:\", e)\n    MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,\n                                                 quantization_config=bnb_config,\n                                                 trust_remote_code=True,\n                                                 device_map=\"auto\")\nprint(\"Model loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T12:23:50.544915Z","iopub.execute_input":"2025-10-06T12:23:50.545187Z","iopub.status.idle":"2025-10-06T12:24:09.645506Z","shell.execute_reply.started":"2025-10-06T12:23:50.545167Z","shell.execute_reply":"2025-10-06T12:24:09.644889Z"}},"outputs":[{"name":"stdout","text":"Loading model (this can take a minute)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e12397bd866a476a88d9e6ba8011b3b2"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d3036a4de4f414dac48cf9b2aa2a2a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88ee823d481b46cdb58258de88e71cc2"}},"metadata":{}},{"name":"stdout","text":"Model loaded.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# CELL 11 — prepare for k-bit + attach LoRA (PEFT)\nmodel = prepare_model_for_kbit_training(model)  # necessary before get_peft_model with 4bit\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],   # common choice that targets attention projections; can add k_proj/o_proj if you like\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # sanity check\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T12:28:05.463603Z","iopub.execute_input":"2025-10-06T12:28:05.464179Z","iopub.status.idle":"2025-10-06T12:28:06.055152Z","shell.execute_reply.started":"2025-10-06T12:28:05.464155Z","shell.execute_reply":"2025-10-06T12:28:06.054291Z"}},"outputs":[{"name":"stdout","text":"trainable params: 1,843,200 || all params: 3,087,781,888 || trainable%: 0.0597\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen2.5-asap-p1-lora\",\n    per_device_train_batch_size=1,        # safe for T4\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=16,       # simulate batch of 16\n    num_train_epochs=1,                   # complete within ~1 hour\n    learning_rate=3e-5,                   # faster convergence since 1 epoch\n    fp16=True,\n    optim=\"paged_adamw_8bit\",             # memory-efficient optimizer\n    logging_steps=25,                     # log every 25 steps\n    eval_strategy=\"steps\",          # evaluate during training\n    eval_steps=200,                       # run eval every 200 steps\n    save_strategy=\"steps\",                # save model periodically\n    save_steps=200,                       # save every 200 steps (~every 10-15 mins)\n    save_total_limit=None,                # keep ALL checkpoints\n    load_best_model_at_end=True,          # loads lowest eval loss checkpoint\n    ddp_find_unused_parameters=False,\n    report_to=\"none\",                     # disable wandb etc\n    logging_dir=\"./logs\",              \n    # store logs\n    warmup_ratio=0.1,                     # small warmup helps stability\n    gradient_checkpointing=True,          # saves VRAM\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T12:35:30.263424Z","iopub.execute_input":"2025-10-06T12:35:30.263681Z","iopub.status.idle":"2025-10-06T12:35:30.308895Z","shell.execute_reply.started":"2025-10-06T12:35:30.263663Z","shell.execute_reply":"2025-10-06T12:35:30.308265Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/123878270.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# CELL 13 — train (this will run on the T4)\ntrainer.train()\n# after training you can save the LoRA adapters only (small)\nmodel.save_pretrained(\"./qwen2.5-asap-p1-lora-adapter\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T12:37:58.804465Z","iopub.execute_input":"2025-10-06T12:37:58.804761Z","iopub.status.idle":"2025-10-06T13:05:07.863451Z","shell.execute_reply.started":"2025-10-06T12:37:58.804740Z","shell.execute_reply":"2025-10-06T13:05:07.862875Z"}},"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [78/78 26:44, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# CELL 14 — evaluation: loss on test set (uses labels prepared earlier)\nmetrics = trainer.evaluate(tokenized[\"test\"])\nprint(\"Eval metrics (from Trainer):\", metrics)\n# metrics should contain eval_loss; if not, you can compute CE loss manually below\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T13:05:17.684472Z","iopub.execute_input":"2025-10-06T13:05:17.685154Z","iopub.status.idle":"2025-10-06T13:07:10.227295Z","shell.execute_reply.started":"2025-10-06T13:05:17.685126Z","shell.execute_reply":"2025-10-06T13:07:10.226659Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='268' max='268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [268/268 01:51]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Eval metrics (from Trainer): {'eval_loss': nan, 'eval_runtime': 112.5344, 'eval_samples_per_second': 2.381, 'eval_steps_per_second': 2.381, 'epoch': 1.0}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel   # <-- THIS WAS MISSING\n\nDEVICE = \"cuda:0\"\n\nMODEL_PATH = \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\"                  # your base model path\nLORA_PATH = \"/kaggle/working/qwen2.5-asap-p1-lora-adapter\"       # your fine-tuned LoRA folder\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH).to(DEVICE)\n\n# Load fine-tuned LoRA weights on top\nmodel = PeftModel.from_pretrained(base_model, LORA_PATH).to(DEVICE)\n\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T13:17:32.874926Z","iopub.execute_input":"2025-10-06T13:17:32.875610Z","iopub.status.idle":"2025-10-06T13:17:35.597703Z","shell.execute_reply.started":"2025-10-06T13:17:32.875580Z","shell.execute_reply":"2025-10-06T13:17:35.596857Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(151936, 2048, padding_idx=151654)\n        (layers): ModuleList(\n          (0-35): 36 x Qwen2DecoderLayer(\n            (self_attn): Qwen2Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n              (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n              (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n        (rotary_emb): Qwen2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# CELL 15 — Efficient test evaluation\nimport re\nimport torch\n\nfrom transformers import logging\nlogging.set_verbosity_error()\n\ndef predict_single_score(essay_text):\n    # Truncate essay to reduce token length\n    prompt = SYSTEM_PROMPT + PROMPT_SUFFIX + essay_text.strip()[:2000] + SCORE_SUFFIX\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).to(DEVICE)\n\n    with torch.no_grad():\n        # Greedy generation\n        generated = model.generate(\n            **inputs,\n            max_new_tokens=8,\n            do_sample=False,               # greedy\n            pad_token_id=tokenizer.pad_token_id\n        )\n\n    gen_text = tokenizer.decode(\n        generated[0][inputs[\"input_ids\"].shape[1]:], \n        skip_special_tokens=True\n    )\n    \n    m = re.search(r\"\\b(\\d{1,2})\\b\", gen_text)\n    if m:\n        return int(m.group(1)), gen_text.strip()\n    else:\n        return None, gen_text.strip()\n\n\n# Process essays in small batches to avoid GPU bottleneck\nbatch_size = 8  # adjust based on GPU memory\ngold = test_df['domain1_score'].tolist()\nessays = test_df['essay'].tolist()\n\npreds = []\nraw_outs = []\n\nfor start in range(0, len(essays), batch_size):\n    batch_essays = essays[start:start+batch_size]\n    for e in batch_essays:\n        p, raw = predict_single_score(e)\n        preds.append(p)\n        raw_outs.append(raw)\n    print(f\"Processed {len(preds)}/{len(essays)} essays\")\n\n\n# Compute exact-match accuracy\ncorrect = sum(1 for a, b in zip(preds, gold) if a is not None and a == b)\ntotal = len(gold)\naccuracy = correct / total\nprint(f\"Test exact-match accuracy: {accuracy:.4f} ({correct}/{total})\")\n\n# Show some examples where model prediction was wrong\nfor i, (g, p, raw) in enumerate(zip(gold, preds, raw_outs)):\n    if i < 10 and p != g:\n        print(\"GOLD\", g, \"PRED\", p, \"RAWOUT\", repr(raw))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T13:33:34.367531Z","iopub.execute_input":"2025-10-06T13:33:34.368047Z","iopub.status.idle":"2025-10-06T13:43:50.665738Z","shell.execute_reply.started":"2025-10-06T13:33:34.368018Z","shell.execute_reply":"2025-10-06T13:43:50.665079Z"}},"outputs":[{"name":"stdout","text":"Processed 8/268 essays\nProcessed 16/268 essays\nProcessed 24/268 essays\nProcessed 32/268 essays\nProcessed 40/268 essays\nProcessed 48/268 essays\nProcessed 56/268 essays\nProcessed 64/268 essays\nProcessed 72/268 essays\nProcessed 80/268 essays\nProcessed 88/268 essays\nProcessed 96/268 essays\nProcessed 104/268 essays\nProcessed 112/268 essays\nProcessed 120/268 essays\nProcessed 128/268 essays\nProcessed 136/268 essays\nProcessed 144/268 essays\nProcessed 152/268 essays\nProcessed 160/268 essays\nProcessed 168/268 essays\nProcessed 176/268 essays\nProcessed 184/268 essays\nProcessed 192/268 essays\nProcessed 200/268 essays\nProcessed 208/268 essays\nProcessed 216/268 essays\nProcessed 224/268 essays\nProcessed 232/268 essays\nProcessed 240/268 essays\nProcessed 248/268 essays\nProcessed 256/268 essays\nProcessed 264/268 essays\nProcessed 268/268 essays\nTest exact-match accuracy: 0.3843 (103/268)\nGOLD 10 PRED 8 RAWOUT '8\\nThe score assigned to the essay'\nGOLD 5 PRED 8 RAWOUT '8\\nThe score provided for the essay'\nGOLD 9 PRED 8 RAWOUT '8\\nThe score assigned to the essay'\nGOLD 6 PRED 8 RAWOUT '8\\nThe score provided is appropriate given'\nGOLD 7 PRED 8 RAWOUT '8\\nThe score provided is appropriate given'\nGOLD 6 PRED 8 RAWOUT '8\\nThe score provided for the essay'\nGOLD 9 PRED 8 RAWOUT '8\\nThe essay provided is quite detailed'\nGOLD 8 PRED 9 RAWOUT '9\\nThe score provided for the essay'\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# CELL 16 — Save results\npd.DataFrame({\"essay\": essays, \"gold\": gold, \"pred\": preds, \"raw_out\": raw_outs}).to_csv(\"test_predictions.csv\", index=False)\nprint(\"Saved test_predictions.csv. Adapter saved in ./qwen2.5-asap-p1-lora-adapter\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T13:43:50.666874Z","iopub.execute_input":"2025-10-06T13:43:50.667114Z","iopub.status.idle":"2025-10-06T13:43:50.697112Z","shell.execute_reply.started":"2025-10-06T13:43:50.667096Z","shell.execute_reply":"2025-10-06T13:43:50.696419Z"}},"outputs":[{"name":"stdout","text":"Saved test_predictions.csv. Adapter saved in ./qwen2.5-asap-p1-lora-adapter\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"!zip -r fine_tuned_model_try2.zip /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:06:45.103106Z","iopub.execute_input":"2025-10-06T14:06:45.103396Z","iopub.status.idle":"2025-10-06T14:06:46.650244Z","shell.execute_reply.started":"2025-10-06T14:06:45.103375Z","shell.execute_reply":"2025-10-06T14:06:46.649361Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/ (stored 0%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/ (stored 0%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/vocab.json","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 69%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/adapter_config.json (deflated 53%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/special_tokens_map.json (deflated 69%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/training_args.bin (deflated 52%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/scheduler.pt (deflated 56%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/README.md (deflated 66%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/merges.txt (deflated 57%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/rng_state.pth (deflated 25%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/trainer_state.json (deflated 61%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/added_tokens.json (deflated 67%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/optimizer.pt (deflated 9%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/scaler.pt (deflated 60%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/tokenizer_config.json (deflated 89%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora/checkpoint-78/chat_template.jinja (deflated 71%)\n  adding: kaggle/working/test_predictions.csv (deflated 68%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora-adapter/ (stored 0%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora-adapter/adapter_config.json (deflated 53%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora-adapter/README.md (deflated 66%)\n  adding: kaggle/working/qwen2.5-asap-p1-lora-adapter/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'fine_tuned_model_try2.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:07:16.528991Z","iopub.execute_input":"2025-10-06T14:07:16.529265Z","iopub.status.idle":"2025-10-06T14:07:16.535564Z","shell.execute_reply.started":"2025-10-06T14:07:16.529242Z","shell.execute_reply":"2025-10-06T14:07:16.534879Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/fine_tuned_model_try2.zip","text/html":"<a href='fine_tuned_model_try2.zip' target='_blank'>fine_tuned_model_try2.zip</a><br>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}