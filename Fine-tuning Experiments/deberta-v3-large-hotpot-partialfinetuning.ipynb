{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q --upgrade pip\n!pip uninstall -y transformers huggingface_hub datasets\n!pip install -q transformers==4.37.2 datasets==2.16.1 huggingface_hub==0.20.3 accelerate==0.27.2 peft==0.7.1\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-09T02:59:46.315096Z","iopub.execute_input":"2025-11-09T02:59:46.315832Z","iopub.status.idle":"2025-11-09T03:01:31.982612Z","shell.execute_reply.started":"2025-11-09T02:59:46.315799Z","shell.execute_reply":"2025-11-09T03:01:31.981843Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hFound existing installation: transformers 4.53.3\nUninstalling transformers-4.53.3:\n  Successfully uninstalled transformers-4.53.3\nFound existing installation: huggingface-hub 0.36.0\nUninstalling huggingface-hub-0.36.0:\n  Successfully uninstalled huggingface-hub-0.36.0\nFound existing installation: datasets 4.4.1\nUninstalling datasets-4.4.1:\n  Successfully uninstalled datasets-4.4.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2023.10.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.2 which is incompatible.\ndiffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.20.3 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.20.3 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import transformers, datasets, huggingface_hub\nprint(\"transformers =\", transformers.__version__)\nprint(\"datasets =\", datasets.__version__)\nprint(\"hub =\", huggingface_hub.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:01:31.984057Z","iopub.execute_input":"2025-11-09T03:01:31.984361Z","iopub.status.idle":"2025-11-09T03:01:39.102653Z","shell.execute_reply.started":"2025-11-09T03:01:31.984336Z","shell.execute_reply":"2025-11-09T03:01:39.102014Z"}},"outputs":[{"name":"stdout","text":"transformers = 4.37.2\ndatasets = 2.16.1\nhub = 0.20.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from IPython.core.display import HTML\nHTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:01:39.103276Z","iopub.execute_input":"2025-11-09T03:01:39.103628Z","iopub.status.idle":"2025-11-09T03:01:39.109236Z","shell.execute_reply.started":"2025-11-09T03:01:39.103612Z","shell.execute_reply":"2025-11-09T03:01:39.108585Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<script>Jupyter.notebook.kernel.restart()</script>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import transformers, datasets, huggingface_hub\nprint(\"transformers =\", transformers.__version__)\nprint(\"datasets =\", datasets.__version__)\nprint(\"hub =\", huggingface_hub.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:01:39.110845Z","iopub.execute_input":"2025-11-09T03:01:39.111089Z","iopub.status.idle":"2025-11-09T03:01:40.369318Z","shell.execute_reply.started":"2025-11-09T03:01:39.111071Z","shell.execute_reply":"2025-11-09T03:01:40.368477Z"}},"outputs":[{"name":"stdout","text":"transformers = 4.37.2\ndatasets = 2.16.1\nhub = 0.20.3\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:01:40.370104Z","iopub.execute_input":"2025-11-09T03:01:40.370393Z","iopub.status.idle":"2025-11-09T03:01:40.379401Z","shell.execute_reply.started":"2025-11-09T03:01:40.370367Z","shell.execute_reply":"2025-11-09T03:01:40.378661Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\nAutoTokenizer,\nAutoModelForQuestionAnswering,\nTrainingArguments,\nTrainer,\ndefault_data_collator\n)\nimport numpy as np\nimport random\nimport collections","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:01:40.380203Z","iopub.execute_input":"2025-11-09T03:01:40.380584Z","iopub.status.idle":"2025-11-09T03:01:55.168188Z","shell.execute_reply.started":"2025-11-09T03:01:40.380558Z","shell.execute_reply":"2025-11-09T03:01:55.167601Z"}},"outputs":[{"name":"stderr","text":"2025-11-09 03:01:45.229833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762657305.418972      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762657305.471587      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"model_name = \"deepset/deberta-v3-large-squad2\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\n\n\n# Freeze all parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the top 4 encoder layers\nunfrozen_layers = [\"layer.20\", \"layer.21\", \"layer.22\", \"layer.23\"]  # DeBERTa has 24 layers (0-23)\n\nfor name, param in model.named_parameters():\n    if any(layer_name in name for layer_name in unfrozen_layers):\n        param.requires_grad = True\n\n# Also unfreeze the QA head (output layer)\nfor name, param in model.qa_outputs.named_parameters():\n    param.requires_grad = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:01:55.168919Z","iopub.execute_input":"2025-11-09T03:01:55.169412Z","iopub.status.idle":"2025-11-09T03:02:04.689363Z","shell.execute_reply.started":"2025-11-09T03:01:55.169392Z","shell.execute_reply":"2025-11-09T03:02:04.688762Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/392 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e8ea1335a734053b49820b18b9a2620"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"166751f3157e4523aea8554855f1d6f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/8.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56417e7064eb4ff18725c81a946bebde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/18.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b451027dc2304adbbd2ab7a0692ca3ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c173a6643a94b52835aa2f02f65dda5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02c57dd84800401190b397e3d879f3bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a182f92836124e3cbfcf328fab341ee1"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable parameters: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:02:04.690080Z","iopub.execute_input":"2025-11-09T03:02:04.690327Z","iopub.status.idle":"2025-11-09T03:02:04.697066Z","shell.execute_reply.started":"2025-11-09T03:02:04.690300Z","shell.execute_reply":"2025-11-09T03:02:04.696045Z"}},"outputs":[{"name":"stdout","text":"Trainable parameters: 50,386,946 / 434,014,210 (11.61%)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Load a smaller subset for faster training\nraw_datasets = load_dataset(\"hotpotqa/hotpot_qa\", \"fullwiki\")\ntrain_ds = raw_datasets[\"train\"].shuffle(seed=42).select(range(45000))\nval_ds = raw_datasets[\"validation\"].shuffle(seed=42).select(range(5000))\n\n\nmax_length = 384\nstride = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:02:04.698048Z","iopub.execute_input":"2025-11-09T03:02:04.698298Z","iopub.status.idle":"2025-11-09T03:02:20.107984Z","shell.execute_reply.started":"2025-11-09T03:02:04.698277Z","shell.execute_reply":"2025-11-09T03:02:20.107405Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa91f049f7624e5e81124e0dd861fd7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/166M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8fe9637aa354ba7a622ee33db0e9abe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/166M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed9deb62e9b145efaefb23384de70718"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/28.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83d4c1030f3f4130b74533122e02b0a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/27.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c0ab7b30869476ca3f9f51ae2846f93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"483005e1d68944ffa2c8599509351e58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3592c5b6b5b74282820c5afddd135972"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7405 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"430b0929461e41dc9139eae205cf6078"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def prepare_train_features(examples):\n    questions = examples[\"question\"]\n    contexts = [\n        \" \".join(sent for para in ctx[\"sentences\"] for sent in para)\n        for ctx in examples[\"context\"]\n    ]\n\n    answers = [ans[\"text\"] if isinstance(ans, dict) else ans for ans in examples[\"answer\"]]\n    \n    \n    tokenized = tokenizer(\n        questions,\n        contexts,\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n\n    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized.pop(\"offset_mapping\")\n    \n    \n    start_positions, end_positions = [], []\n    for i, offsets in enumerate(offset_mapping):\n        sample_idx = sample_mapping[i]\n        answer = answers[sample_idx]\n        context = contexts[sample_idx]\n        cls_index = tokenized[\"input_ids\"][i].index(tokenizer.cls_token_id)\n\n\n        if not answer or not str(answer).strip():\n            start_positions.append(cls_index)\n            end_positions.append(cls_index)\n            continue\n        \n        \n        start_char = context.find(answer)\n        if start_char == -1:\n            start_positions.append(cls_index)\n            end_positions.append(cls_index)\n            continue\n        \n        \n        end_char = start_char + len(answer)\n        sequence_ids = tokenized.sequence_ids(i)\n        \n        \n        token_start_index = 0\n        while sequence_ids[token_start_index] != 1:\n            token_start_index += 1\n\n\n        token_end_index = len(sequence_ids) - 1\n        while sequence_ids[token_end_index] != 1:\n            token_end_index -= 1\n        \n        \n        while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n            token_start_index += 1\n        token_start_index -= 1\n        \n        \n        token_end_index_tmp = token_start_index\n        while token_end_index_tmp < len(offsets) and offsets[token_end_index_tmp][1] < end_char:\n            token_end_index_tmp += 1\n        token_end_index_tmp -= 1\n\n\n        if token_start_index < 0 or token_end_index_tmp < 0:\n            start_positions.append(cls_index)\n            end_positions.append(cls_index)\n        else:\n            start_positions.append(token_start_index)\n            end_positions.append(token_end_index_tmp)\n\n\n    tokenized[\"start_positions\"] = start_positions\n    tokenized[\"end_positions\"] = end_positions\n    return tokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:02:20.109922Z","iopub.execute_input":"2025-11-09T03:02:20.110155Z","iopub.status.idle":"2025-11-09T03:02:20.118370Z","shell.execute_reply.started":"2025-11-09T03:02:20.110138Z","shell.execute_reply":"2025-11-09T03:02:20.117812Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def prepare_validation_features(examples):\n    questions = examples[\"question\"]\n    example_ids = examples[\"id\"]\n    contexts = [\n        \" \".join(sent for para in ctx[\"sentences\"] for sent in para)\n        for ctx in examples[\"context\"]\n    ]\n\n\n    tokenized = tokenizer(\n        questions,\n        contexts,\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n    tokenized[\"example_id\"] = [example_ids[sample_mapping[i]] for i in range(len(tokenized[\"input_ids\"]))]\n\n    # Dummy labels (required for evaluation to run)\n    tokenized[\"start_positions\"] = [0] * len(tokenized[\"input_ids\"])\n    tokenized[\"end_positions\"] = [0] * len(tokenized[\"input_ids\"])\n\n    new_offsets = []\n    for i, offsets in enumerate(tokenized[\"offset_mapping\"]):\n        sequence_ids = tokenized.sequence_ids(i)\n        new_offsets.append([o if sequence_ids[k] == 1 else None for k, o in enumerate(offsets)])\n    tokenized[\"offset_mapping\"] = new_offsets\n\n    return tokenized\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:02:20.118983Z","iopub.execute_input":"2025-11-09T03:02:20.119161Z","iopub.status.idle":"2025-11-09T03:02:21.354325Z","shell.execute_reply.started":"2025-11-09T03:02:20.119148Z","shell.execute_reply":"2025-11-09T03:02:21.353357Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_dataset = train_ds.map(prepare_train_features, batched=True, remove_columns=train_ds.column_names, num_proc=2)\neval_dataset = val_ds.map(prepare_validation_features, batched=True, remove_columns=val_ds.column_names, num_proc=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:02:21.355355Z","iopub.execute_input":"2025-11-09T03:02:21.355650Z","iopub.status.idle":"2025-11-09T03:04:43.531791Z","shell.execute_reply.started":"2025-11-09T03:02:21.355626Z","shell.execute_reply":"2025-11-09T03:04:43.530884Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/45000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd7ef19462e544cbb5463cef2f537892"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce773ef569024c609108bc486e8f6dbe"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Only pass model-ready fields\ndef strip_eval(features):\n    keep = [\"input_ids\", \"attention_mask\"]\n    if \"token_type_ids\" in features.features:\n        keep.append(\"token_type_ids\")\n    return features.remove_columns([col for col in features.column_names if col not in keep])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:04:43.532823Z","iopub.execute_input":"2025-11-09T03:04:43.533081Z","iopub.status.idle":"2025-11-09T03:04:43.537777Z","shell.execute_reply.started":"2025-11-09T03:04:43.533058Z","shell.execute_reply":"2025-11-09T03:04:43.537107Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import numpy as np\nimport collections\n\ndef compute_hotpot_em_f1(pred_dict, references):\n    def normalize_text(s):\n        import re, string\n        s = s.lower().strip()\n        s = ''.join(ch for ch in s if ch not in string.punctuation)\n        s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n        return ' '.join(s.split())\n\n    def f1(pred, truth):\n        pred_toks = normalize_text(pred).split()\n        truth_toks = normalize_text(truth).split()\n        common = collections.Counter(pred_toks) & collections.Counter(truth_toks)\n        if not common:\n            return 0.0\n        prec = sum(common.values()) / len(pred_toks)\n        rec = sum(common.values()) / len(truth_toks)\n        return 2 * prec * rec / (prec + rec + 1e-12)\n\n    id_to_ans = {ex[\"id\"]: ex[\"answer\"] for ex in references}\n    em, f1_sum, total = 0, 0, 0\n    for id_, pred in pred_dict.items():\n        if id_ in id_to_ans:\n            truth = id_to_ans[id_]\n            em += normalize_text(pred) == normalize_text(truth)\n            f1_sum += f1(pred, truth)\n            total += 1\n    return {\"eval_em\": em / total, \"eval_f1\": f1_sum / total} if total else {\"eval_em\": 0.0, \"eval_f1\": 0.0}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:04:43.538479Z","iopub.execute_input":"2025-11-09T03:04:43.538711Z","iopub.status.idle":"2025-11-09T03:04:43.906090Z","shell.execute_reply.started":"2025-11-09T03:04:43.538691Z","shell.execute_reply":"2025-11-09T03:04:43.905492Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def postprocess_qa_predictions(examples, features, start_logits, end_logits):\n    example_id_to_index = {k[\"id\"]: i for i, k in enumerate(examples)}\n    pred = {}\n    features_per_example = collections.defaultdict(list)\n    for i, f in enumerate(features):\n        features_per_example[f[\"example_id\"]].append(i)\n\n    for ex_id, feat_ids in features_per_example.items():\n        prelim = []\n        for fid in feat_ids:\n            start, end = start_logits[fid], end_logits[fid]\n            offsets = features[fid][\"offset_mapping\"]\n            s_ids = np.argsort(start)[-10:][::-1]\n            e_ids = np.argsort(end)[-10:][::-1]\n            for s in s_ids:\n                for e in e_ids:\n                    if s >= len(offsets) or e >= len(offsets): continue\n                    if offsets[s] is None or offsets[e] is None: continue\n                    if e < s or e - s + 1 > 30: continue\n                    score = start[s] + end[e]\n                    prelim.append((score, fid, s, e))\n        if not prelim:\n            pred[ex_id] = \"\"\n            continue\n        best = max(prelim, key=lambda x: x[0])\n        _, fid, s, e = best\n        offset = features[fid][\"offset_mapping\"]\n        ctx = \" \".join(sent for para in examples[example_id_to_index[ex_id]][\"context\"][\"sentences\"] for sent in para)\n        pred[ex_id] = ctx[offset[s][0]:offset[e][1]] if offset[s] and offset[e] else \"\"\n    return pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:04:43.907057Z","iopub.execute_input":"2025-11-09T03:04:43.907277Z","iopub.status.idle":"2025-11-09T03:04:43.917446Z","shell.execute_reply.started":"2025-11-09T03:04:43.907254Z","shell.execute_reply":"2025-11-09T03:04:43.916869Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def build_metrics():\n    def fn(p):\n        preds = postprocess_qa_predictions(\n            val_ds,\n            eval_dataset,\n            np.array(p.predictions[0]),\n            np.array(p.predictions[1])\n        )\n        scores = compute_hotpot_em_f1(preds, val_ds)\n\n        # ✅ Include eval_loss directly if available\n        if hasattr(p, \"loss\") and p.loss is not None:\n            scores[\"eval_loss\"] = p.loss\n\n        return scores\n    return fn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:04:43.918232Z","iopub.execute_input":"2025-11-09T03:04:43.918520Z","iopub.status.idle":"2025-11-09T03:04:43.930221Z","shell.execute_reply.started":"2025-11-09T03:04:43.918498Z","shell.execute_reply":"2025-11-09T03:04:43.929621Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from transformers import TrainerCallback\n\nclass DebugLossLogger(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs:\n            print(f\"[Step {state.global_step}] ➤ {logs}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:04:43.930895Z","iopub.execute_input":"2025-11-09T03:04:43.931163Z","iopub.status.idle":"2025-11-09T03:04:43.941167Z","shell.execute_reply.started":"2025-11-09T03:04:43.931142Z","shell.execute_reply":"2025-11-09T03:04:43.940398Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./debarta-squad-hotpot\",\n    evaluation_strategy=\"steps\",\n    eval_steps=3500,                     # Evaluate frequently (for test)\n    logging_steps=500,                  # Log loss every 10 steps\n    logging_first_step=True,          # Log step 1\n    save_steps=7000,\n    save_total_limit=1,\n\n    num_train_epochs=1,               # Or 1, if short run\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n\n    learning_rate=5e-6,\n    fp16=True,\n    dataloader_num_workers=2,\n    dataloader_pin_memory=True,\n\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_f1\",  # matches your metric dict\n    greater_is_better=True,           # for f1 metric\n\n    logging_dir=\"./logs\",\n    log_level=\"info\",\n    report_to=\"none\",                 # Avoids WandB or TensorBoard setup issues\n\n    disable_tqdm=False                # Show training progress bar\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:27:51.541025Z","iopub.execute_input":"2025-11-09T03:27:51.541731Z","iopub.status.idle":"2025-11-09T03:27:51.548413Z","shell.execute_reply.started":"2025-11-09T03:27:51.541704Z","shell.execute_reply":"2025-11-09T03:27:51.547687Z"}},"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=default_data_collator,\n    compute_metrics=build_metrics(),\n    callbacks=[DebugLossLogger()] \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:27:53.996742Z","iopub.execute_input":"2025-11-09T03:27:53.997041Z","iopub.status.idle":"2025-11-09T03:27:54.011730Z","shell.execute_reply.started":"2025-11-09T03:27:53.997019Z","shell.execute_reply":"2025-11-09T03:27:54.011044Z"}},"outputs":[{"name":"stderr","text":"Using auto half precision backend\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:27:56.066467Z","iopub.execute_input":"2025-11-09T03:27:56.067222Z","iopub.status.idle":"2025-11-09T03:28:04.599069Z","shell.execute_reply.started":"2025-11-09T03:27:56.067196Z","shell.execute_reply":"2025-11-09T03:28:04.598046Z"}},"outputs":[{"name":"stderr","text":"***** Running training *****\n  Num examples = 222,115\n  Num Epochs = 1\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 8\n  Total optimization steps = 13,882\n  Number of trainable parameters = 50,386,946\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7' max='13882' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    7/13882 00:05 < 4:23:20, 0.88 it/s, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"[Step 1] ➤ {'loss': 1.6796, 'learning_rate': 4.999639821351391e-06, 'epoch': 0.0}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1539\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1872\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m                 ):\n\u001b[1;32m   1876\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"trainer.save_model(\"./deberta-v3-large-squad-v2-hotpot\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf ./debarta-squad-hotpot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:28:13.657372Z","iopub.execute_input":"2025-11-09T03:28:13.657938Z","iopub.status.idle":"2025-11-09T03:28:13.843873Z","shell.execute_reply.started":"2025-11-09T03:28:13.657909Z","shell.execute_reply":"2025-11-09T03:28:13.842838Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"!zip -r deberta_fine_tuned_model.zip /kaggle/working/deberta-v3-large-squad-v2-hotpot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:23:18.680299Z","iopub.execute_input":"2025-11-09T03:23:18.681147Z","iopub.status.idle":"2025-11-09T03:23:18.871639Z","shell.execute_reply.started":"2025-11-09T03:23:18.681102Z","shell.execute_reply":"2025-11-09T03:23:18.870885Z"}},"outputs":[{"name":"stdout","text":"\tzip warning: name not matched: /kaggle/working/deberta-v3-large-squad-v2-hotpot\n\nzip error: Nothing to do! (try: zip -r deberta_fine_tuned_model.zip . -i /kaggle/working/deberta-v3-large-squad-v2-hotpot)\n","output_type":"stream"}],"execution_count":25}]}